import asyncio
import json
import logging
import re
import urllib.parse
from typing import Any, Dict
from urllib.parse import unquote

import requests
import websockets
from fastmcp import Client

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)

# Define LLM server URLs
LLM_SERVER_BASE_URL = "http://localhost:8000"
LLM_SERVER_WS_URL = "ws://0.0.0.0:8000/ws/llm"


# ws://0.0.0.0:8000/ws/llm/+12345952496/348c06e4-8ded-49b0-a353-90fc15bf94a4
async def configure_session(
    owner_id: str, kb_id: str, config: dict, system_prompt: str
):
    """
    Configures the LLM session by sending a POST request to the /configure endpoint.
    Returns the session_id generated by the server.
    """
    session_config = {
        "kb_id": kb_id,
        "owner_id": owner_id,
        "config": config,
        "system_prompt": system_prompt,
    }
    try:
        response = requests.post(
            f"{LLM_SERVER_BASE_URL}/configure", json=session_config, timeout=10
        )
        response.raise_for_status()
        response_data = response.json()
        session_id = response_data.get("session_id")
        if not session_id:
            logging.error("No session_id returned from server")
            return None
        logging.info(
            f"Session configured for owner_id: {owner_id}, session_id: {session_id}"
        )
        return session_id
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to configure session: {e}")
        return None


def parse_llm_output(raw_output: str) -> Dict[str, Any]:
    """
    Extracts ALL valid tool calls from the response text.
    Skips malformed or invalid JSON blocks.
    Returns 'tool_calls' type if at least one valid call found.
    Otherwise, returns 'text' type with error info.
    """
    start_tag = "<tool_call>"
    end_tag = "<tool_call>"

    # Find all tool call blocks
    tool_call_blocks = re.findall(
        rf"{re.escape(start_tag)}\s*(.*?)\s*{re.escape(end_tag)}",
        raw_output,
        re.DOTALL,
    )

    valid_tool_calls = []
    errors = []

    for i, block in enumerate(tool_call_blocks):
        try:
            tool_call_data = json.loads(block)
            if (
                "function_name" not in tool_call_data
                or "arguments" not in tool_call_data
            ):
                raise KeyError("Missing 'function_name' or 'arguments'")
            valid_tool_calls.append(
                {
                    "name": tool_call_data["function_name"],
                    "arguments": tool_call_data["arguments"],
                }
            )
        except (json.JSONDecodeError, KeyError, TypeError) as e:
            errors.append(f"Block {i + 1}: {str(e)}")

    # Decide what to return
    if valid_tool_calls:
        result = {
            "type": "tool_calls",
            "tool_calls": valid_tool_calls,
        }
        if errors:
            result["parsing_errors"] = errors  # optional diagnostic info
        return result
    else:
        error_detail = "; ".join(errors) if errors else "No valid tool calls found"
        return {
            "type": "text",
            "content": raw_output,
            "error": error_detail,
        }


async def chat_with_llm(owner_id: str, session_id: str, user_input: str):
    """
    Connects to the LLM WebSocket and sends a user message, handling the response.
    Returns the final message containing 'choices'.
    """
    session_key = f"{owner_id}:{session_id}"
    websocket_url = f"{LLM_SERVER_WS_URL}/{owner_id}/{session_id}"
    try:
        async with websockets.connect(
            websocket_url, open_timeout=5, ping_timeout=20, close_timeout=5
        ) as websocket:
            logging.info(f"Connected to LLM WebSocket for {session_key}")

            context = "No information retrieved"

            message = json.dumps(
                {
                    "owner_id": owner_id,
                    "user_input": user_input,
                    "retrieved_data": context,
                }
            )

            await websocket.send(message)
            logging.debug(f"Sent to LLM: {message}")

            # Receive and process responses until we get one with 'choices'
            while True:
                response = await websocket.recv()
                logging.debug(f"Raw LLM Stream Chunk: {response[:200]}...")

                try:
                    data = json.loads(response)
                    if data.get("choices"):
                        logging.info("âœ… Received final LLM response with choices")
                        return data
                except json.JSONDecodeError:
                    logging.warning("Received non-JSON response from LLM, skipping...")
                    continue

    except websockets.exceptions.ConnectionClosedError as e:
        logging.error(f"WebSocket connection failed for {session_key}: {e}")
        return {"error": f"LLM server connection failed: {str(e)}"}
    except Exception as e:
        logging.error(f"Error in WebSocket communication: {e}")
        return {"error": str(e)}


async def llm(user_input: str, max_tool_iterations: int = 3):
    """
    Main LLM interaction loop:
    - Configures session
    - Sends user input
    - Parses output â†’ if tool calls, executes them
    - Optionally sends tool results back to LLM for next step
    - Repeats up to max_tool_iterations
    - Returns final text response or last tool result
    """
    owner_id = "+12345952496"
    kb_id = "kb+12345952496_en"
    config = {"temperature": 0.7, "max_tokens": 3000}
    system_prompt = """
You are a helpful assistant with access to external tools to assist in answering user queries. Use a tool only when strictly necessary to provide an accurate and complete response. If you can answer the query directly with your knowledge, do not use a toolâ€”respond with the final answer in plain text.

### Available Tools:
{{TOOLS}}

Each tool is defined within <tool> tags as a JSON object with:
- **name**: The tool's identifier (e.g., 'code_execution', 'search').
- **description**: What the tool does.
- **inputSchema**: A JSON schema specifying required and optional parameters.
- **outputSchema**: The expected output format (typically a JSON string).

### Tool Call Instructions:
When a tool is necessary:
- Output **only** the tool call(s) using the mandatory <tool_call> and <tool_call> tags, with no additional text, explanations, or partial answers.
- Use the following format for each tool call, ensuring both opening <tool_call> and closing <tool_call> tags are included:
  <tool_call>
  {"function_name": "tool_name", "arguments": {"param1": "value1", "param2": "value2"}}
  <tool_call>
- **function_name**: Must match the tool's name exactly (e.g., 'code_execution', 'search').
- **arguments**: A JSON object with keys and values strictly adhering to the tool's inputSchema. Include all required parameters and relevant optional ones. Values must match the schema types (e.g., strings, integers, arrays).
- Example for the 'search' tool:
  <tool_call>
  {"function_name": "search", "arguments": {"query": "What is the price of usb hub?", "num_results": 5}}
  <tool_call>
- For multiple tool calls, output consecutive <tool_call> blocks, each wrapped in its own <tool_call> and <tool_call> tags.
- Do not escape values; provide them as plain text within the JSON structure.
- Ensure the JSON is valid and fully complies with the tool's inputSchema.
- Always include both the opening <tool_call> and closing <tool_call> tags for every tool call; never omit the closing <tool_call> tag.

### Response Instructions:
- **If a tool is needed**: Output only the <tool_call> block(s) as described, with no other content, ensuring each block has both <tool_call> and <tool_call> tags.
- **If no tool is needed**: Respond directly with the answer in plain text, without any <tool_call> or <tool_call> tags.
- **After tool results**: When tool results are provided (e.g., in a subsequent message as JSON), incorporate them into your reasoning and provide a final answer in plain text, unless further tool calls are required.
- Ensure all tool calls are properly wrapped in <tool_call> and <tool_call> tags, and never include partial answers or explanations outside of these tags when a tool is invoked.
"""
    tools_list = await return_tools()
    tools_str = "\n\n".join(tools_list)
    system_prompt = system_prompt.replace("{{TOOLS}}", tools_str)

    # Configure the session and get the session_id
    session_id = await configure_session(owner_id, kb_id, config, system_prompt)
    if not session_id:
        logging.error("Exiting due to configuration failure")
        return None, False

    logging.info(f"Session ID: {session_id}")

    current_input = user_input
    iteration = 0

    while iteration < max_tool_iterations:
        iteration += 1
        logging.info(f"Iteration {iteration}: Sending to LLM: {current_input}")

        # Send message and get response
        response = await chat_with_llm(owner_id, session_id, current_input)
        if "error" in response:
            logging.error(f"LLM Error: {response['error']}")
            return f"LLM Error: {response['error']}", False

        # Extract message content
        message_content = (
            response.get("choices", [{}])[0].get("message", {}).get("content", "")
        )
        if not message_content.strip():
            logging.warning("Empty LLM response")
            return "No response from LLM", False

        parsed_response = parse_llm_output(message_content)
        # for err in parsed_response["parsing_errors"]:
        #     logging.warning(f"Tool block parsing error: {err}")
        logging.info(f"Parsed Response Type: {parsed_response.get('type')}")

        # If it's text â†’ we're done!
        if parsed_response.get("type") == "text":
            final_text = parsed_response.get("content", "").strip()
            if final_text:
                logging.info("âœ… Final Answer from LLM (no tool needed)")
                return final_text, False
            else:
                error_msg = parsed_response.get("error", "Unknown error")
                logging.warning(f"LLM returned text with error: {error_msg}")
                return f"LLM Error: {error_msg}", False

        # If it's tool_calls â†’ execute them
        elif parsed_response.get("type") == "tool_calls":
            tool_calls = parsed_response.get("tool_calls", [])
            tool_results = []

            async with Client("http://localhost:6000/mcp") as client:
                for i, tool_call in enumerate(tool_calls):
                    tool_name = tool_call.get("name")
                    tool_args = tool_call.get("arguments", {})

                    logging.info(
                        f"âž¡ï¸  Executing tool {i+1}/{len(tool_calls)}: {tool_name} with args: {tool_args}"
                    )
                    try:
                        result = await client.call_tool(tool_name, tool_args)
                        tool_results.append(
                            {
                                "tool_name": tool_name,
                                "arguments": tool_args,
                                "result": result,
                            }
                        )
                        logging.info(f"âœ… Tool {tool_name} executed successfully")
                    except Exception as e:
                        error_detail = f"Tool '{tool_name}' failed: {str(e)}"
                        logging.error(error_detail)
                        tool_results.append(
                            {
                                "tool_name": tool_name,
                                "arguments": tool_args,
                                "error": str(e),
                            }
                        )

            # Format tool results for LLM
            tool_feedback = "\n".join(
                f"Tool '{tr['tool_name']}' called with {tr['arguments']} returned: {tr.get('result', 'ERROR: ' + tr.get('error', 'Unknown'))}"
                for tr in tool_results
            )

            current_input = f"""
Previous user query: {user_input}

Tool execution results:
{tool_feedback}

Based on these results, please provide a final answer to the user's original question.
Do NOT call any more tools unless absolutely necessary.
If you must call another tool, use the <tool_call> tags again.
Otherwise, respond with plain text only.
"""
            logging.info("Sending tool results back to LLM for synthesis...")

        else:
            logging.error(f"Unknown response type: {parsed_response.get('type')}")
            return "Unknown LLM response type", False

    # If we hit max iterations
    return "Max tool iterations reached. Could not produce final answer.", False


async def return_tools():
    """
    Connects to the MCP server and retrieves the list of available tools.
    """

    async with Client("http://localhost:6000/mcp") as client:
        tools_list = []
        tools = await client.list_tools()
        for tool in tools:
            # Optionally, format tools for system prompt
            tool_str = (
                f"Tool(name='{tool.name}', title={tool.title}, "
                f"description='{tool.description}', inputSchema={tool.inputSchema}, "
                f"outputSchema={tool.outputSchema}, annotations={tool.annotations}, "
                f"meta={tool.meta})"
            )
            tools_list.append(f"<tool>\n{tool_str}\n</tool>")
        return tools_list


async def main():
    # user_input = "Hi there! Can you tell me a joke about programming?"
    user_input = "What is the price of usb hub? Use the search tool to find the answer."

    print("=" * 60)
    print(f"User: {user_input}")
    print("=" * 60)

    result, tool_used = await llm(user_input)

    print("\n" + "=" * 60)
    if tool_used:
        print("ðŸ”§ Final Tool Result:")
    else:
        print("ðŸ’¬ Final LLM Response:")
    print(result)
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
