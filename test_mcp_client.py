# test_mcp_client.py
from fastmcp import Client
import asyncio

import asyncio
import json
import logging

import requests
import websockets

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)

# Define LLM server URLs
LLM_SERVER_BASE_URL = "http://localhost:5002"
LLM_SERVER_WS_URL = "ws://localhost:5002/v1/llm"


async def configure_session(
    owner_id: str, kb_id: str, config: dict, system_prompt: str
):
    """
    Configures the LLM session by sending a POST request to the /configure endpoint.
    Returns the session_id generated by the server.
    """
    session_config = {
        "kb_id": kb_id,
        "owner_id": owner_id,
        "config": config,
        "system_prompt": system_prompt,
    }
    print(session_config)
    try:
        response = requests.post(
            f"{LLM_SERVER_BASE_URL}/configure", json=session_config, timeout=10
        )

        response.raise_for_status()
        response_data = response.json()
        session_id = response_data.get("session_id")
        if not session_id:
            logging.error("No session_id returned from server")
            return None
        logging.info(
            f"Session configured for owner_id: {owner_id}, session_id: {session_id}"
        )
        return session_id
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to configure session: {e}")
        return None


async def chat_with_llm(owner_id: str, session_id: str, user_input: str):
    """
    Connects to the LLM WebSocket and sends a user message, handling the response.
    """
    session_key = f"{owner_id}:{session_id}"
    websocket_url = f"{LLM_SERVER_WS_URL}/{owner_id}/{session_id}"

    try:
        async with websockets.connect(
            websocket_url, open_timeout=5, ping_timeout=20, close_timeout=5
        ) as websocket:
            logging.info(f"Connected to LLM WebSocket for {session_key}")

            # Use static context as requested
            context = "No information retrieved"

            # Format the message as expected by the LLM service
            message = json.dumps(
                {
                    "owner_id": owner_id,
                    "user_input": user_input,
                    "retrieved_data": context,
                }
            )

            await websocket.send(message)
            logging.info(f"Sent to LLM: {message}")

            # Receive and process responses
            while True:
                response = await websocket.recv()
                logging.info(f"Received from LLM: {response}")

                try:
                    data = json.loads(response)
                    if data.get("choices"):
                        return data
                except json.JSONDecodeError:
                    continue

    except websockets.exceptions.ConnectionClosedError as e:
        logging.error(f"WebSocket connection failed for {session_key}: {e}")
        return {"error": f"LLM server connection failed: {str(e)}"}
    except Exception as e:
        logging.error(f"Error in WebSocket communication: {e}")
        return {"error": str(e)}


async def llm(user_input = "Tell me a joke about programming."):
    """
    Main function to demonstrate LLM WebSocket communication.
    """
    owner_id = "+12345952496"
    kb_id = "kb+12345952496_en"
    config = {"temperature": 0.7, "max_tokens": 3000}
    system_prompt = "You are a helpful assistant with."

    # Configure the session and get the session_id
    session_id = await configure_session(owner_id, kb_id, config, system_prompt)
    if not session_id:
        logging.error("Exiting due to configuration failure")
        return

    # Send a message and get the response
    response = await chat_with_llm(owner_id, session_id, user_input)
    print("LLM Response:", json.dumps(response, indent=2))


async def main():
    # ‚úÖ Use root endpoint for streamable-http
    async with Client("http://localhost:6000/mcp") as client:
        tools = await client.list_tools()
        print(f"Available tools: {tools}")

        # Test the search tool
        query = "pillow price?"
        result = await client.call_tool("search", {"query": query})
        print(f"\nüîç Search Result:\n{result}")

        prompt = "Summarize the output of the retrieval based on the user's query\nQuery: {} Retrieved: {}."
        summary = await client.call_tool("agent", {"prompt": prompt.format(query, result)})
        print(f"\nüìù Summary:\n{summary}")


if __name__ == "__main__":
    asyncio.run(main())